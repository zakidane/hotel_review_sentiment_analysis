{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Zaki Kidane**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.** First, counts of each word in training file with tag 1 and 0 are recorded separately (both (w_i, 0) and w_i, 1) counts) in dictionaries *positive_count* and *negative_count*. Next, total number of words with 1 and 0 are recorded in integer variables ( count(w,0) and count(w,1)). \n",
    "After getting the relevant counts, p(w<sub>i</sub>|c=0) and p(w<sub>i</sub>|c=1) are stored in dictionaries with keys as w<sub>i</sub> (here Laplace smoothing was used). <br>\n",
    "The following values were calculated as indicated below:<br>\n",
    "P(c=0) = count(w,0) / (count(w,0) + count(w,1)) <br>\n",
    "P(c=1) = count(w,1) / (count(w,0) + count(w,1)) <br>\n",
    "\n",
    "Once we have all that we need the probability can be calculated as follows: <br>\n",
    "P(C|text)=P(w<sub>1</sub>|C)*P(w<sub>2</sub>|C)*...*P(w<sub>n</sub>|C)*P(C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** To handle the situation when a word is present in one class but missing in other class, words were added to counts of both classes, but if they don't exist in one of the classes, their counts there would be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.** If a word is missing in both classes, it is simply ignored when calculating the score function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D.** The precision, recall and f1 were calculated based on the relevant Noteboook day in our class with the formulas as follows: <br> \n",
    "   precision = true_pos/(true_pos+false_pos) <br>\n",
    "   recall = true_pos/(true_pos+false_neg) <br> \n",
    "   f1 = (2\\*prec\\*rec)/(rec+prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E.** The same calculation/formulas were used for rprecision, recall and f1. Since these are objective ways of measuring accuracy, they were not tampered with for the SentimentAnalysisImproved class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F.** In the main SentimentAnalysis class, no proprocessing was used. The sentences were simply tokenized into a list of words by using the split() function. See response to **(G)** for the preprocessing used in the SentimentAnalysisImproved class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**G.** Since using libraries for the classification part was not allowed, none were used here - the only improvement attempted was at the pre-processing level (in the *featurize()* function). The following methods were used and they are listed along with the results: <br> \n",
    "   \\- Changing all words to lowercase using lower() function showed no improvement on results. <br>\n",
    "   \\- Using nltk WordNetLemmatizer() was used to lemmatize the words with no improvement. <br>\n",
    "   \\- Next, spacy preprocessing library was utilized to remove stop_words (common words like 'I', 'is' were removed) with no results\n",
    "   \\- Finally, spacy's lemmatization function was used in the *featurize()* function with no improvement. <br>\n",
    "   \n",
    "   The reasoning behind using lowercase is that words signifying positive and negative should not be put into different dictionary counts as \"liked\" and \"Liked\" are both indicators of positive statements. The rreasoning for lemmatizing using both nltk and spacy was pretty much the same; \"like\" and \"liked\" should both be counted as the same. But the reason there was no improvement was that we still count the total counts of positive/negative so it doesn't matter whether those two strings are in the same dictionary bin or not. <br>\n",
    "   The reason I attempted removing stop words was that they are largely irrelevant and are not likely to indicate whether a sentence is positive or negative (for instance, the word \"I\" is not really associated with either positive and negative comments). However, since stop words exist in both positive and negative counts, their effects here must have cancelled out since I got no improvement by using this method. Overall, I got no improvement on all the pre-processing methods but learned that the effects of lemmatization and stop words are insignificant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
